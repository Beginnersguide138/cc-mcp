import asyncio
import json
from unittest.mock import AsyncMock, patch
from main import MCPServer


async def test_basic_functionality():
    """Test basic MCP server functionality with mocked LLM calls"""
    
    # Create server instance
    server = MCPServer()
    
    # Mock the LLM API calls to avoid actual API requests
    with patch.object(server.intent_classifier, 'classify_intent') as mock_classifier, \
         patch.object(server, '_call_main_llm') as mock_main_llm:
        
        # Setup mocks
        mock_classifier.return_value = AsyncMock()
        mock_classifier.return_value.intent = ["PROBLEM_DEFINITION"]
        mock_classifier.return_value.reason = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå•é¡Œã‚’å®šç¾©ã—ã¦ã„ã‚‹"
        
        mock_main_llm.return_value = "ã“ã‚“ã«ã¡ã¯ï¼ãŠæ‰‹ä¼ã„ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚"
        
        # Test 1: Process a problem definition message
        print("ğŸ§ª Testing problem definition message...")
        result = await server.process_message("AIã§è­°äº‹éŒ²ã‚’è‡ªå‹•è¦ç´„ã—ãŸã„ã§ã™")
        
        print(f"âœ… Response: {result['response']}")
        print(f"âœ… Intent: {result['metadata']['intent_classification']['intent']}")
        print(f"âœ… Reason: {result['metadata']['intent_classification']['reason']}")
        
        # Test 2: Add a constraint
        mock_classifier.return_value.intent = ["CONSTRAINT_ADDITION"]
        mock_classifier.return_value.reason = "åˆ¶ç´„ã‚’è¿½åŠ ã—ã¦ã„ã‚‹"
        mock_main_llm.return_value = "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«é™å®šã—ã¾ã™ã­ã€‚"
        
        print("\nğŸ§ª Testing constraint addition...")
        result = await server.process_message("ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«é™å®šã—ã¦ãã ã•ã„")
        
        print(f"âœ… Response: {result['response']}")
        print(f"âœ… Intent: {result['metadata']['intent_classification']['intent']}")
        
        # Test 3: Check context stats
        print("\nğŸ§ª Testing context stats...")
        stats = server.prompt_engine.get_context_stats()
        print(f"âœ… Has core problem: {stats['has_core_problem']}")
        print(f"âœ… Evolving items count: {stats['evolving_items_count']}")
        print(f"âœ… Recent messages count: {stats['recent_messages_count']}")
        
        # Test 4: Export context
        print("\nğŸ§ª Testing context export...")
        exported = await server.get_context_export()
        context_data = json.loads(exported)
        print(f"âœ… Exported context keys: {list(context_data.keys())}")
        
        # Test 5: Clear context
        print("\nğŸ§ª Testing context clear...")
        cleared = await server.clear_context()
        print(f"âœ… Context cleared: {cleared}")
        
        stats_after_clear = server.prompt_engine.get_context_stats()
        print(f"âœ… Has core problem after clear: {stats_after_clear['has_core_problem']}")
        
    await server.close()
    print("\nâœ¨ All tests completed successfully!")


async def test_prompt_synthesis():
    """Test prompt synthesis with actual context data"""
    
    print("\nğŸ§ª Testing prompt synthesis...")
    
    # Create components
    from context_store import HierarchicalContextStore, Message
    from prompt_synthesis import PromptSynthesisEngine
    
    context_store = HierarchicalContextStore()
    prompt_engine = PromptSynthesisEngine(context_store)
    
    # Add some test data
    context_store.store_message(
        "AIã§è­°äº‹éŒ²ã‚’è‡ªå‹•è¦ç´„ã—ãŸã„",
        ["PROBLEM_DEFINITION"],
        "user"
    )
    
    context_store.store_message(
        "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«é™å®š",
        ["CONSTRAINT_ADDITION"],
        "user"
    )
    
    context_store.store_message(
        "æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦è­°äº‹éŒ²è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ã‚’ææ¡ˆã—ã¾ã™ã€‚",
        ["RESPONSE"],
        "assistant"
    )
    
    # Test prompt synthesis
    current_message = "å…·ä½“çš„ã«ã¯ã©ã®ãƒ¢ãƒ‡ãƒ«ãŒãŠã™ã™ã‚ã§ã™ã‹ï¼Ÿ"
    synthesized = prompt_engine.synthesize_prompt(current_message)
    
    print("âœ… Synthesized prompt:")
    print("-" * 50)
    print(synthesized)
    print("-" * 50)
    
    # Test debug info
    debug_info = prompt_engine.create_debug_info(current_message)
    print(f"âœ… Debug info keys: {list(debug_info.keys())}")
    print(f"âœ… Prompt length: {debug_info['output']['prompt_length']}")


def run_tests():
    """Run all tests"""
    print("ğŸš€ Starting CC-MCP Server Tests...\n")
    
    async def run_all():
        await test_basic_functionality()
        await test_prompt_synthesis()
    
    asyncio.run(run_all())


if __name__ == "__main__":
    run_tests()