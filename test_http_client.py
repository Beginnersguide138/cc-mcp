#!/usr/bin/env python3
"""
Direct HTTP client test for the MCP server to verify functionality
bypassing the MCP interface layer.
"""

import asyncio
import json
import httpx
from dotenv import load_dotenv

load_dotenv()

async def test_server_directly():
    """Test the MCP server functionality directly via HTTP"""
    
    print("ğŸ§ª Testing MCP Server via Direct HTTP Requests")
    print("=" * 60)
    
    # Import and test components directly
    from intent_classifier import IntentClassifier
    from context_store import HierarchicalContextStore
    from prompt_synthesis import PromptSynthesisEngine
    import os
    
    # Test 1: Intent Classifier
    print("\n1ï¸âƒ£ Testing Intent Classification...")
    classifier = IntentClassifier(
        api_url=os.getenv("CLASSIFIER_API_URL"),
        api_key=os.getenv("CLASSIFIER_API_KEY"),
        model=os.getenv("CLASSIFIER_MODEL")
    )
    
    test_message = "AIã§è­°äº‹éŒ²ã‚’è‡ªå‹•è¦ç´„ã—ãŸã„ã§ã™"
    intent_result = await classifier.classify_intent(test_message)
    print(f"âœ… Intent: {intent_result.intent}")
    print(f"âœ… Reason: {intent_result.reason}")
    
    # Test 2: Context Store
    print("\n2ï¸âƒ£ Testing Context Store...")
    context_store = HierarchicalContextStore()
    
    # Store the user message based on intent
    context_store.store_message(
        content=test_message,
        intent_labels=intent_result.intent,
        role="user"
    )
    
    print(f"âœ… Core Context has problem: {context_store.core.get_problem() is not None}")
    print(f"âœ… Evolving Context: {len(context_store.evolving.constraints + context_store.evolving.refinements)} items")
    print(f"âœ… Turn Context: {len(context_store.turn.messages)} items")
    
    # Test 3: Prompt Synthesis
    print("\n3ï¸âƒ£ Testing Prompt Synthesis...")
    prompt_engine = PromptSynthesisEngine(context_store)
    synthesized_prompt = prompt_engine.synthesize_prompt(test_message)
    
    print("âœ… Synthesized Prompt:")
    print("-" * 40)
    print(synthesized_prompt[:500] + "..." if len(synthesized_prompt) > 500 else synthesized_prompt)
    print("-" * 40)
    
    # Test 4: Main LLM Call (simulate)
    print("\n4ï¸âƒ£ Testing Main LLM Integration...")
    
    payload = {
        "model": os.getenv("MAIN_MODEL", "gpt-4"),
        "messages": [
            {"role": "user", "content": synthesized_prompt}
        ],
        "max_tokens": 500,
        "temperature": float(os.getenv("MAIN_TEMPERATURE", "0.7"))
    }
    
    headers = {
        "Authorization": f"Bearer {os.getenv('MAIN_API_KEY')}",
        "Content-Type": "application/json"
    }
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                os.getenv("MAIN_API_URL"),
                json=payload,
                headers=headers,
                timeout=30.0
            )
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result["choices"][0]["message"]["content"].strip()
                print(f"âœ… Main LLM Response (first 200 chars): {ai_response[:200]}...")
                
                # Store AI response in context
                context_store.store_message(
                    content=ai_response,
                    intent_labels=["RESPONSE"],
                    role="assistant"
                )
                
            else:
                print(f"âŒ Main LLM API Error: {response.status_code}")
                print(f"Response: {response.text}")
                
        except Exception as e:
            print(f"âŒ Main LLM Error: {str(e)}")
    
    # Test 5: Context Export/Import
    print("\n5ï¸âƒ£ Testing Context Export/Import...")
    exported_context = context_store.export_state()
    print(f"âœ… Exported context length: {len(exported_context)} characters")
    
    # Test import
    new_store = HierarchicalContextStore()
    new_store.import_state(exported_context)
    print(f"âœ… Imported - Core: {new_store.core.get_problem() is not None}, Evolving: {len(new_store.evolving.constraints + new_store.evolving.refinements)}, Turn: {len(new_store.turn.messages)}")
    
    # Test 6: Multiple turns simulation
    print("\n6ï¸âƒ£ Testing Multi-turn Conversation...")
    
    # Add a constraint
    constraint_message = "äºˆç®—ã¯æœˆ5ä¸‡å††ä»¥å†…ã§ãŠé¡˜ã„ã—ã¾ã™"
    constraint_intent = await classifier.classify_intent(constraint_message)
    print(f"âœ… Constraint intent: {constraint_intent.intent}")
    
    context_store.store_message(
        content=constraint_message,
        intent_labels=constraint_intent.intent,
        role="user"
    )
    
    # Generate new prompt with constraint
    new_prompt = prompt_engine.synthesize_prompt("å…·ä½“çš„ãªå®Ÿè£…æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„")
    print(f"âœ… New prompt includes constraint: {'äºˆç®—' in new_prompt}")
    print(f"âœ… New prompt includes original goal: {'è­°äº‹éŒ²' in new_prompt}")
    
    await classifier.close()
    
    print("\nğŸ‰ All core functionality tests completed successfully!")
    print("ğŸ“‹ The MCP server implementation is working correctly.")
    print("ğŸ”§ The MCP interface layer may need additional debugging for Cline integration.")

if __name__ == "__main__":
    asyncio.run(test_server_directly())
