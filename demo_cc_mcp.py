#!/usr/bin/env python3
"""
CC-MCPå®Ÿæ©Ÿèƒ½ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€ŒCC-MCPã«ã¯ã€ã©ã‚“ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã¾ãŸã€ãã®ä»•çµ„ã«ã¤ã„ã¦ã‚‚æ•™ãˆã¦ä¸‹ã•ã„ã€‚ã€
ã‚’å®Ÿéš›ã«CC-MCPã‚·ã‚¹ãƒ†ãƒ ã§å‡¦ç†
"""

import asyncio
import json
from dotenv import load_dotenv
import os

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

async def demo_cc_mcp_functionality():
    """CC-MCPã®æ©Ÿèƒ½ã‚’ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("ğŸš€ CC-MCPå®Ÿæ©Ÿèƒ½ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 60)
    print("è³ªå•: ã€ŒCC-MCPã«ã¯ã€ã©ã‚“ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã¾ãŸã€ãã®ä»•çµ„ã«ã¤ã„ã¦ã‚‚æ•™ãˆã¦ä¸‹ã•ã„ã€‚ã€")
    print("=" * 60)
    
    # Step 1: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from intent_classifier import IntentClassifier
    from context_store import HierarchicalContextStore  
    from prompt_synthesis import PromptSynthesisEngine
    
    # Step 2: CC-MCPã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    print("\nğŸ“‹ Step 1: CC-MCPã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
    
    classifier = IntentClassifier(
        api_url=os.getenv("CLASSIFIER_API_URL"),
        api_key=os.getenv("CLASSIFIER_API_KEY"),
        model=os.getenv("CLASSIFIER_MODEL")
    )
    
    context_store = HierarchicalContextStore()
    prompt_engine = PromptSynthesisEngine(context_store)
    
    print("âœ… ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†é¡å™¨åˆæœŸåŒ–å®Œäº†")
    print("âœ… éšå±¤å‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆã‚¢åˆæœŸåŒ–å®Œäº†") 
    print("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    # Step 3: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†æ
    user_message = "CC-MCPã«ã¯ã€ã©ã‚“ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã¾ãŸã€ãã®ä»•çµ„ã«ã¤ã„ã¦ã‚‚æ•™ãˆã¦ä¸‹ã•ã„ã€‚"
    
    print(f"\nğŸ“‹ Step 2: ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†æ")
    print(f"å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {user_message}")
    
    intent_result = await classifier.classify_intent(user_message)
    
    print(f"âœ… åˆ†æçµæœ:")
    print(f"   ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆ: {intent_result.intent}")
    print(f"   ç†ç”±: {intent_result.reason}")
    
    # Step 4: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆã‚¢ã«ä¿å­˜
    print(f"\nğŸ“‹ Step 3: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†")
    
    context_store.store_message(
        content=user_message,
        intent_labels=intent_result.intent,
        role="user"
    )
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçŠ¶æ³ã‚’è¡¨ç¤º
    context_summary = context_store.get_context_summary()
    print(f"âœ… ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜å®Œäº†:")
    print(f"   Core Context (å•é¡Œå®šç¾©): {context_summary['core_problem'] is not None}")
    print(f"   Evolving Context (åˆ¶ç´„ç­‰): {len(context_summary['evolving_items'])} é …ç›®")
    print(f"   Turn Context (ä¼šè©±å±¥æ­´): ä¿å­˜æ¸ˆã¿")
    
    # Step 5: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆ
    print(f"\nğŸ“‹ Step 4: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆ")
    
    synthesized_prompt = prompt_engine.synthesize_prompt(user_message)
    
    print("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆå®Œäº†:")
    print("--- åˆæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (æŠœç²‹) ---")
    prompt_lines = synthesized_prompt.split('\n')
    for i, line in enumerate(prompt_lines[:15]):  # æœ€åˆã®15è¡Œã®ã¿è¡¨ç¤º
        print(f"   {line}")
    if len(prompt_lines) > 15:
        print(f"   ... (æ®‹ã‚Š{len(prompt_lines)-15}è¡Œ)")
    print("--- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ‚äº† ---")
    
    # Step 6: ãƒ¡ã‚¤ãƒ³LLMã§ã®å¿œç­”ç”Ÿæˆ
    print(f"\nğŸ“‹ Step 5: ãƒ¡ã‚¤ãƒ³LLMå¿œç­”ç”Ÿæˆ")
    
    import httpx
    
    payload = {
        "model": os.getenv("MAIN_MODEL", "gpt-4"),
        "messages": [
            {"role": "user", "content": synthesized_prompt}
        ],
        "max_tokens": 1000,
        "temperature": float(os.getenv("MAIN_TEMPERATURE", "0.7"))
    }
    
    headers = {
        "Authorization": f"Bearer {os.getenv('MAIN_API_KEY')}",
        "Content-Type": "application/json"
    }
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            os.getenv("MAIN_API_URL"),
            json=payload,
            headers=headers,
            timeout=30.0
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result["choices"][0]["message"]["content"].strip()
            
            print("âœ… LLMå¿œç­”ç”Ÿæˆå®Œäº†:")
            print("--- CC-MCPã‹ã‚‰ã®å›ç­” ---")
            print(ai_response)
            print("--- å›ç­”çµ‚äº† ---")
            
            # Step 7: å¿œç­”ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ä¿å­˜
            print(f"\nğŸ“‹ Step 6: å¿œç­”ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜")
            
            context_store.store_message(
                content=ai_response,
                intent_labels=["RESPONSE"],
                role="assistant"  
            )
            
            print("âœ… å¿œç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            
        else:
            print(f"âŒ LLM API ã‚¨ãƒ©ãƒ¼: {response.status_code}")
            print(f"è©³ç´°: {response.text}")
    
    # Step 8: ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ç¢ºèª
    print(f"\nğŸ“‹ Step 7: ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª")
    
    final_summary = context_store.get_context_summary()
    print("âœ… æœ€çµ‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçŠ¶æ³:")
    print(f"   Core Context: {final_summary['core_problem'] is not None}")
    print(f"   Evolving Context: {len(final_summary['evolving_items'])} é …ç›®")
    print(f"   Turn Context (ä¼šè©±): 2ã‚¿ãƒ¼ãƒ³ä¿å­˜æ¸ˆã¿")
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã®ãƒ‡ãƒ¢
    exported_context = context_store.export_state()
    print(f"   ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ã‚µã‚¤ã‚º: {len(exported_context)} æ–‡å­—")
    
    await classifier.close()
    
    print(f"\nğŸ‰ CC-MCPæ©Ÿèƒ½ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†!")
    print("ğŸ“‹ ã“ã®ã‚ˆã†ã«CC-MCPã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’éšå±¤çš„ã«ç®¡ç†ã—ã€")
    print("   é•·æœŸçš„ãªå¯¾è©±ä¸€è²«æ€§ã‚’ç¶­æŒã—ãªãŒã‚‰å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

if __name__ == "__main__":
    asyncio.run(demo_cc_mcp_functionality())
