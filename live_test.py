#!/usr/bin/env python3
"""
CC-MCP Live Functionality Test
é•·æœŸå¯¾è©±ä¸€è²«æ€§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ†ã‚¹ãƒˆ
"""
import asyncio
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from intent_classifier import IntentClassifier
from context_store import HierarchicalContextStore
from prompt_synthesis import PromptSynthesisEngine
import httpx

class CCMCPLiveTest:
    def __init__(self):
        # Initialize components
        self.context_store = HierarchicalContextStore()
        self.prompt_engine = PromptSynthesisEngine(self.context_store)
        
        classifier_api_url = os.getenv("CLASSIFIER_API_URL", "https://api.openai.com/v1/chat/completions")
        classifier_api_key = os.getenv("CLASSIFIER_API_KEY", "")
        classifier_model = os.getenv("CLASSIFIER_MODEL", "gpt-3.5-turbo")
        
        self.intent_classifier = IntentClassifier(
            api_url=classifier_api_url,
            api_key=classifier_api_key,
            model=classifier_model
        )
        
        # Main LLM configuration
        self.main_api_url = os.getenv("MAIN_API_URL", "https://api.openai.com/v1/chat/completions")
        self.main_api_key = os.getenv("MAIN_API_KEY", "")
        self.main_model = os.getenv("MAIN_MODEL", "gpt-4")
        self.main_temperature = float(os.getenv("MAIN_TEMPERATURE", "0.7"))
        
        self.http_client = httpx.AsyncClient()
    
    async def process_message(self, user_message: str, session_id: str = "default"):
        """Process a user message through the complete MCP pipeline."""
        try:
            print(f"ğŸ”„ Processing: {user_message}")
            
            # Step 1: Classify intent
            intent_result = await self.intent_classifier.classify_intent(user_message)
            print(f"ğŸ“Š Intent: {intent_result.intent} - {intent_result.reason}")
            
            # Step 2: Store in context based on intent
            self.context_store.store_message(
                content=user_message,
                intent_labels=intent_result.intent,
                role="user"
            )
            
            # Step 3: Synthesize prompt with context
            synthesized_prompt = self.prompt_engine.synthesize_prompt(user_message)
            
            # Step 4: Generate response from main LLM
            main_response = await self._call_main_llm(synthesized_prompt)
            
            # Step 5: Store assistant response in turn context
            self.context_store.store_message(
                content=main_response,
                intent_labels=["RESPONSE"],
                role="assistant"
            )
            
            print(f"ğŸ’¬ Response: {main_response[:200]}...")
            print(f"ğŸ“ˆ Context Stats: {self.prompt_engine.get_context_stats()}")
            print("-" * 60)
            
            return {
                "response": main_response,
                "metadata": {
                    "intent_classification": {
                        "intent": intent_result.intent,
                        "reason": intent_result.reason
                    },
                    "context_stats": self.prompt_engine.get_context_stats(),
                    "session_id": session_id
                }
            }
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            return {
                "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                "metadata": {"error": str(e), "session_id": session_id}
            }
    
    async def _call_main_llm(self, prompt: str) -> str:
        """Call the main LLM with the synthesized prompt"""
        payload = {
            "model": self.main_model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 2000,
            "temperature": self.main_temperature
        }
        
        headers = {
            "Authorization": f"Bearer {self.main_api_key}",
            "Content-Type": "application/json"
        }
        
        response = await self.http_client.post(
            self.main_api_url,
            json=payload,
            headers=headers
        )
        response.raise_for_status()
        
        result = response.json()
        return result["choices"][0]["message"]["content"].strip()
    
    async def close(self):
        """Clean up resources"""
        await self.intent_classifier.close()
        await self.http_client.aclose()

async def main():
    """Interactive CC-MCP test session"""
    print("ğŸš€ CC-MCP Live Functionality Test")
    print("=" * 60)
    print("é•·æœŸå¯¾è©±ä¸€è²«æ€§ã®ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...")
    print()
    
    test_system = CCMCPLiveTest()
    
    # Test scenario: æ®µéšçš„ãªè¦æ±‚ã®ç™ºå±•
    test_messages = [
        "Webã‚¢ãƒ—ãƒªã®é–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å§‹ã‚ãŸã„ã§ã™ã€‚Vue.jsã‚’ä½¿ã„ãŸã„ã¨æ€ã„ã¾ã™ã€‚",
        "ãŸã ã—ã€äºˆç®—ã¯10ä¸‡å††ä»¥å†…ã§ã€æœŸé–“ã¯2ãƒ¶æœˆä»¥å†…ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚",
        "å®Ÿã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼æ©Ÿèƒ½ã‚‚å¿…è¦ã§ã™ã€‚ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
        "æœ€åˆã®è³ªå•ã«æˆ»ã‚Šã¾ã™ãŒã€ã©ã®ã‚ˆã†ãªæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ãŒãŠã™ã™ã‚ã§ã™ã‹ï¼Ÿ"
    ]
    
    try:
        for i, message in enumerate(test_messages, 1):
            print(f"ğŸ”· ã‚¿ãƒ¼ãƒ³ {i}: å¯¾è©±ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ")
            await test_system.process_message(message, "live_test_session")
            print()
            
            # Show evolving context
            stats = test_system.prompt_engine.get_context_stats()
            print(f"ğŸ“‹ ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçŠ¶æ³:")
            print(f"   Core Context: {'è¨­å®šæ¸ˆã¿' if stats['has_core_problem'] else 'æœªè¨­å®š'}")
            print(f"   Evolving Context: {stats['evolving_items_count']} é …ç›®")
            print(f"   Turn Context: {stats['recent_messages_count']} ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
            print()
            
            if i < len(test_messages):
                input("â³ æ¬¡ã®ã‚¿ãƒ¼ãƒ³ã«é€²ã‚€ã«ã¯ Enter ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
                print()
    
    finally:
        await test_system.close()
        print("ğŸ ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    asyncio.run(main())
